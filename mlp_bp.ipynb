{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP) with Backpropagation (BP) algorithm\n",
    "\n",
    "Implementation of a multilayer perceptron with backpropagation algorithm for classification of data contained in a csv file, in which there is an input layer, a hidden layer with n neurons and an output layer with k neurons.\n",
    "\n",
    "## Perceptron multicapa con algoritmo de retropropagación\n",
    "\n",
    "Implementación de un perceptron multicapa con algoritmo de retropropagación para la clasificación de datos contenidos en un archivo csv., en la cual se tiene una capa de entrada, una capa oculta con n neuronas y una capa de salida con k neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Callable, NewType\n",
    "\n",
    "# types\n",
    "Array = NewType('Array', np.ndarray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "#### Funciones de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar functions\n",
    "# Please execute this cell before running the rest of the notebook\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Sigmoid of x\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Sigmoid derivative function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Sigmoid derivative of x\n",
    "\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Hyperbolic tangent of x\n",
    "\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent derivative function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Hyperbolic tangent derivative of x\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Rectified Linear Unit of x\n",
    "\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit derivative function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Rectified Linear Unit derivative of x\n",
    "\n",
    "    \"\"\"\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Softmax of x\n",
    "\n",
    "    \"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    \"\"\"\n",
    "    Softmax derivative function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Input value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array_like\n",
    "        Softmax derivative of x\n",
    "\n",
    "    \"\"\"\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "list_activation_functions = [sigmoid, tanh, relu, softmax]\n",
    "list_activation_functions_derivatives = [sigmoid_derivative, tanh_derivative, relu_derivative, softmax_derivative]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, W1: np.ndarray, W2: np.ndarray, af_hidden: Callable, af_output: Callable\n",
    "            ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts the output of a neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input data\n",
    "    W1 : np.ndarray\n",
    "        Weights of the first layer\n",
    "    W2 : np.ndarray\n",
    "        Weights of the second layer\n",
    "    activation_function : str\n",
    "        Activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Predicted output\n",
    "\n",
    "    \"\"\"\n",
    "    # Forward propagation\n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = af_hidden(Z1)\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    A2 = af_output(Z2)\n",
    "    return A2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation functions\n",
    "\n",
    "#### Funciones de manipulación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(x: np.ndarray, y: np.ndarray, train_percentage=0.8):\n",
    "    \"\"\"\n",
    "    Separates data into training and test sets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Input data\n",
    "    y : numpy.ndarray\n",
    "        Output data\n",
    "    train_percentage : float, optional\n",
    "        Percentage of data to be used for training. The default is 0.8.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train : numpy.ndarray\n",
    "        Training data\n",
    "    y_train : numpy.ndarray\n",
    "        Training output data\n",
    "    x_test : numpy.ndarray\n",
    "        Test data\n",
    "    y_test : numpy.ndarray\n",
    "        Test output data\n",
    "\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    n_samples = x.shape[0]\n",
    "    # Number of training samples\n",
    "    n_train = int(n_samples * train_percentage)\n",
    "    # Number of test samples\n",
    "    # n_test = n_samples - n_train\n",
    "    # Training data\n",
    "    x_train = x[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    # Test data\n",
    "    x_test = x[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    " \n",
    "# The csv files are in data folder, with names EarthSpace.csv and MedSci.csv\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the data from the csv files\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        Input data\n",
    "    y : np.ndarray\n",
    "        Output data\n",
    "\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df_earthspace = pd.read_csv(\"data/EarthSpace.csv\", header=None) # shape (389,512)\n",
    "    df_medsci = pd.read_csv(\"data/MedSci.csv\", header=None) # shape (330,512)\n",
    "    # Concatenate data\n",
    "    df = pd.concat([df_earthspace, df_medsci])\n",
    "    # Input data\n",
    "    x = df.to_numpy()\n",
    "    # Add bias\n",
    "    x = np.insert(x, 0, 1, axis=1)\n",
    "    # Build output data\n",
    "    y = np.zeros((x.shape[0], 2))\n",
    "    y[:df_earthspace.shape[0], 0] = 1\n",
    "    y[df_earthspace.shape[0]:, 1] = 1\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_bp(n: int, k: int, x: np.ndarray, y: np.ndarray,\n",
    "           lr=0.001, epochs=1000, af_hidden_index=0, af_output_index=3,\n",
    "           print_loss=True,  print_validation_loss=True,\n",
    "           ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron with backpropagation algorithm, using stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "    ----------------------------\n",
    "        n: Number of neurons in the hidden layer.\n",
    "        k: Number of classes.\n",
    "        x: Input data.\n",
    "        y: Target data.\n",
    "        lr: Learning rate.\n",
    "        epochs: Number of epochs.\n",
    "\n",
    "    Returns:\n",
    "    ----------------------------\n",
    "        w1: Weights for the hidden layer.\n",
    "        w2: Weights for the output layer.\n",
    "        convergence: List of the loss values for each epoch.\n",
    "        validation_convergence: List of the validation loss values for each epoch.\n",
    "    \"\"\"\n",
    "    convergence = []\n",
    "    validation_convergence = []\n",
    "\n",
    "    # Separate data into training and test sets\n",
    "    x_train, y_train, x_test, y_test = separate_data(x, y)\n",
    "\n",
    "    # Activation functions\n",
    "    af_hidden = list_activation_functions[af_hidden_index]\n",
    "    af_output = list_activation_functions[af_output_index]\n",
    "    # Activation functions derivatives\n",
    "    af_hidden_derivative = list_activation_functions_derivatives[af_hidden_index]\n",
    "    af_output_derivative = list_activation_functions_derivatives[af_output_index]\n",
    "\n",
    "    # Initialize weights\n",
    "    w1 = np.random.randn(x.shape[1], n) # weights for the hidden layer\n",
    "    w2 = np.random.randn(n, k) # weights for the output layer\n",
    "    for _ in range(epochs):\n",
    "        for example, target in zip(x_train, y_train):\n",
    "            # Forward propagation\n",
    "            z1 = w1 @ example\n",
    "            a1 = af_hidden(z1)\n",
    "            z2 = np.dot(w2, a1)\n",
    "            a2 = af_output(z2)\n",
    " \n",
    "            # Backpropagation\n",
    "            delta2 = (a2 - target) * af_output_derivative(z2) # Output layer\n",
    "            delta1 = np.dot(w2, delta2) * af_hidden_derivative(z1) # Hidden layer\n",
    "\n",
    "            # Update weights\n",
    "            w2 -= lr * np.outer(a1, delta2)\n",
    "            w1 -= lr * np.outer(example, delta1)\n",
    "\n",
    "        # Save the loss\n",
    "        loss = np.mean(np.square(y - y_pred))\n",
    "        convergence.append(loss)\n",
    "        if print_loss:\n",
    "            print(f\"Loss: {np.mean(np.square(y - y_pred))}\")\n",
    "\n",
    "        # Save the validation loss\n",
    "        y_pred = predict(x_test, w1, w2, af_hidden, af_output)\n",
    "        validation_loss = np.mean(np.square(y_test - y_pred))\n",
    "        validation_convergence.append(validation_loss)\n",
    "        if print_validation_loss:\n",
    "            print(f\"Validation loss: {validation_loss}\")\n",
    "\n",
    "    return w1, w2, convergence, validation_convergence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m x, y \u001b[39m=\u001b[39m load_data()\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m w1, w2, convergence, validation_convergence \u001b[39m=\u001b[39m mlp_bp(\u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m, x, y, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, af_hidden_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, af_output_index\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m        print_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  print_validation_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Plot loss\u001b[39;00m\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mplot(convergence, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining loss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mmlp_bp\u001b[0;34m(n, k, x, y, lr, epochs, af_hidden_index, af_output_index, print_loss, print_validation_loss)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[1;32m     49\u001b[0m delta2 \u001b[39m=\u001b[39m (a2 \u001b[39m-\u001b[39m target) \u001b[39m*\u001b[39m af_output_derivative(z2) \u001b[39m# Output layer\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m delta1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(w2, delta2) \u001b[39m*\u001b[39m af_hidden_derivative(z1) \u001b[39m# Hidden layer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# Update weights\u001b[39;00m\n\u001b[1;32m     53\u001b[0m w2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mouter(a1, delta2)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x, y = load_data()\n",
    "# Train model\n",
    "w1, w2, convergence, validation_convergence = mlp_bp(3, 2, x, y, lr=0.001, epochs=1000, af_hidden_index=0, af_output_index=3,\n",
    "       print_loss=True,  print_validation_loss=True)\n",
    "# Plot loss\n",
    "plt.plot(convergence, label=\"Training loss\")\n",
    "plt.plot(validation_convergence, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Predict\n",
    "y_pred = predict(x, w1, w2, list_activation_functions[0], list_activation_functions[3])\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
